"""
Calculate metrics for evaluating retrieval systems and ranking search results.
"""

from enum import Enum
from typing import Optional
from pydantic import BaseModel, Field


class TestConfiguration(BaseModel):
    """Test configuration."""

    embedding_model: str = "text-embedding-3-large"
    """The embedding model to use."""

    distance_function: str = "cosine"
    """The distance function to use."""


class MetricsRetrieval(BaseModel):
    """
    Used both for single result and aggregated metrics for retrieval systems.
    """

    normalized_discounted_cumulative_gain: float
    """Normalized Discounted Cumulative Gain."""

    normalized_cumulative_gain: float
    """
    Normalized Cumulative Gain where the formula is tweaked to be suitable for RAG systems.
    - multiply relevance with the novelty, gain is only achieved for novel results

    Instead of normalizing using the INCG we use the max relevance value times k
    Alternatively have a number N (N > k) of search results, and sort the N results when calculating INCG for k
    """

    # diversity: float
    # """Diversity of the search results."""

    relevance: float
    """Relevance of the search results."""





class GainsCalculationMethod(Enum):
    """Enum for the gains calculation method."""

    RELEVANCE_ONLY = "relevance_only"
    RELEVANCE_AND_DIVERSITY = "relevance_and_diversity"


class ResultItemMetrics(BaseModel):
    """
    Metrics for a single search result.
    """

    # gain: float
    # """
    # A combination of relevance, diversity, used_in_rag_result and produced_answer_alone.

    # It should represent how useful the search result is for the RAG system to improve its results.
    # """
    # diversity: float
    # """Diversity of the search results."""

    relevance: float
    """Relevance of the search results."""

    def calculate_gain(self, method: GainsCalculationMethod) -> float:
        """Calculate the gain for the search result."""
        if method == GainsCalculationMethod.RELEVANCE_ONLY:
            return self.relevance
        # elif method == GainsCalculationMethod.RELEVANCE_AND_DIVERSITY:
        #     if self.relevance > 0.3:
        #         return self.relevance * self.diversity
        #     else:
        #         return 0.0
        else:
            raise ValueError(f"Unknown method: {method}")


class RankedSearchResult(BaseModel):
    """
    A single search result.
    """

    document_content: str
    """The document that was retrieved."""

    rank: int
    """The rank of the search result."""

    vector_embeddings: list[float] = Field(default_factory=list)
    """The vector embeddings of the search result."""

    # used_in_rag_result: Optional[bool] = None
    # """Whether the search result was used as a citation in the RAG result."""

    # produced_answer_alone: Optional[bool] = None
    # """Whether the search result produced an answer in RAG on its own."""

    metrics: Optional[ResultItemMetrics] = None
    """
    Metrics for the search result. This will be calculated in a separate step. Will be None if not calculated yet.
    """


class QueryResult(BaseModel):
    """
    A query result.
    """

    query: str
    """The query that was used to retrieve the document."""

    # answer: str
    # """The answer that was generated by RAG prompt, based on query and search results."""

    search_results: list[RankedSearchResult] = Field(default_factory=list)
    """The search results."""


class SearchEvaluationRun(BaseModel):
    """
    A run of the retrieval system.
    """

    test_configuration: TestConfiguration
    """The test configuration."""

    search_results: list[QueryResult] = Field(default_factory=list)
    """The search results."""

    retrieval_metrics: Optional[MetricsRetrieval] = None
    """The metrics for the retrieval system."""

    def summary(self) -> str:
        ...

    def plot_comparison(self) -> None:
        ...


if __name__ == "__main__":
    import erdantic as erd

    erd.draw(SearchEvaluationRun, out="diagram2.png")
