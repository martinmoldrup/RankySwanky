from langchain_core.language_models import BaseChatModel
import os
import pydantic
from rankyswanky.metrics.abstract_retrieved_document_metrics import RelevanceEvaluatorBase

from rankyswanky.models.retrieval_evaluation_models import RetrievedDocumentMetrics
from rankyswanky.adapters import llm

SYSTEM_PROMPT: str = """
You are an expert evaluator of search results with a deep understanding of many various topics.
You should rank the relevance by giving star ratings from 1 to 5, where 1 is not relevant at all and 5 is highly relevant.

5 means that the context is perfect and a full answer to the question can be generated by using this context.
4 means that the context is very relevant and can be used to generate a good answer to the question, but it may not cover all aspects of the question.
3 means that the context is relevant, but it may not provide enough information to generate a complete answer to the question.
2 means that the context is somewhat relevant, but it does not provide enough information to generate a good answer to the question.
1 means that the context is completely irrelevant to the question and does not provide any useful information for the question.

This is the question:
{question}

This is the context:
{context}

How relevant is the context to the question?
"""

class EvaluationResult(pydantic.BaseModel):
    """Model to hold the evaluation result."""

    relevance_score_1_to_5: int
    """Relevance score from 1 to 5."""



class RelevanceEvaluator(RelevanceEvaluatorBase):
    def __init__(self) -> None:
        self._open_chat_llm: BaseChatModel = llm.chat_llm
        # self._embeddings_llm: AzureOpenAIEmbeddings = llm.embeddings_llm
        self._question: str = ""

    def set_question(self, question: str) -> None:
        """Set the question for the relevance evaluator."""
        self._question = question

    def get_relevance_score(self, context: str) -> int:
        """Get the relevance score of the context to the question using an LLM."""
        evaluation_result: EvaluationResult = self._open_chat_llm.with_structured_output(EvaluationResult).invoke(
            SYSTEM_PROMPT.format(question=self._question, context=context),
        )
        return evaluation_result.relevance_score_1_to_5

    def create_retrieved_document_metrics(self, context: str) -> RetrievedDocumentMetrics | None:
        """Create retrieved document metrics from the context."""
        relevance_score = self.get_relevance_score(context)
        normalized_relevance = (relevance_score - 1) / 4 if relevance_score is not None else 0.0
        if relevance_score is not None:
            return RetrievedDocumentMetrics(
                relevance=normalized_relevance,
                novelty=0.0,
            )
        return None

if __name__ == "__main__":
    evaluator = RelevanceEvaluator()
    # Example usage
    question = "What is the capital of France?"
    context = "The capital of France is Paris."
    evaluator.set_question(question)
    score = evaluator.get_relevance_score(context)
    print(
        f"Relevance score for the question '{question}' with context '{context}': {score}"
    )

    not_relevant_context = "The capital of Germany is Berlin."
    not_relevant_score = evaluator.get_relevance_score(question, not_relevant_context)
    print(
        f"Relevance score for the question '{question}' with context '{not_relevant_context}': {not_relevant_score}"
    )